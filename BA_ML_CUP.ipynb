{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ENVIRONMENT SETTINGS"
      ],
      "metadata": {
        "id": "Q3AmLKzSWet3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "File Operations"
      ],
      "metadata": {
        "id": "cjFclACIV3Cn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkkvXm9CVuOT"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!mkdir dataset\n",
        "!cp -r /content/drive/MyDrive/analytics_cup ./dataset"
      ],
      "metadata": {
        "id": "ht6WGKJ8V6C8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Global Variable Definitions"
      ],
      "metadata": {
        "id": "ilFtMXsrWW4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "❗️Please do not forget to change the WORKING DIR variable to run the following cells correctly. "
      ],
      "metadata": {
        "id": "RmhOQctiXcFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 44\n",
        "WORKING_DIR = '/content/dataset/analytics_cup/'"
      ],
      "metadata": {
        "id": "lElR9wkwWQnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install required libraries"
      ],
      "metadata": {
        "id": "OcxNeVlBqSKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost"
      ],
      "metadata": {
        "id": "RJMoeWIrqT7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Required Python Libraries"
      ],
      "metadata": {
        "id": "e77TFijGWqkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Kg2R6osLWpvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA PREPERATION"
      ],
      "metadata": {
        "id": "V06fwSOCWxH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the all of the .csv files into Pandas dataframes."
      ],
      "metadata": {
        "id": "v6jXDoEXY22S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "business_units = pd.read_csv(os.path.join(WORKING_DIR, \"business_units.csv\"))\n",
        "customers = pd.read_csv(os.path.join(WORKING_DIR,\"customers.csv\"))\n",
        "sales_orders = pd.read_csv(os.path.join(WORKING_DIR,\"sales_orders.csv\"))\n",
        "sales_orders_header = pd.read_csv(os.path.join(WORKING_DIR,\"sales_orders_header.csv\"))\n",
        "service_map = pd.read_csv(os.path.join(WORKING_DIR,\"service_map.csv\"))\n",
        "classification = pd.read_csv(os.path.join(WORKING_DIR,\"classification.csv\"))"
      ],
      "metadata": {
        "id": "6U-1cwP_Wyre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manipulating obtained dataframes so that we have one final dataframe that includes all of the information provided. "
      ],
      "metadata": {
        "id": "h1ms1F-AZB4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a unique tracking id to track each customer\n",
        "classification['track_id'] = classification.index+1\n",
        "\n",
        "# Recalculate net value from Sales_Order.csv file since the provided ones are not correct\n",
        "df_sales_orders_correct_net_values = pd.DataFrame(sales_orders.groupby('Sales_Order')['Net_Value'].sum())\n",
        "df_sales_orders_correct_net_values = df_sales_orders_correct_net_values.rename(columns={'Net_Value': 'Net_Value_Corrected'})\n",
        "sales_orders_header = sales_orders_header.merge(df_sales_orders_correct_net_values, on=['Sales_Order'], how='left')\n",
        "\n",
        "# We do not need the 'Net_Value' column anymore\n",
        "sales_orders_header.drop('Net_Value', axis=1, inplace= True)\n",
        "\n",
        "# Join all of the information provided within the .csv files \n",
        "anti_join_customers = customers.merge(sales_orders, on=['Sales_Order', 'Item_Position'], how='left', indicator=True)\n",
        "anti_join_customers = anti_join_customers[anti_join_customers['_merge'] == 'left_only']\n",
        "\n",
        "anti_join_saleOrders = sales_orders.merge(customers, on=['Sales_Order', 'Item_Position'], how='left', indicator=True)\n",
        "anti_join_saleOrders = anti_join_saleOrders[anti_join_saleOrders['_merge'] == 'left_only']\n",
        "\n",
        "index_anti_join_customers = anti_join_customers.index\n",
        "index_anti_join_saleOrders = anti_join_saleOrders.index\n",
        "\n",
        "customers.loc[index_anti_join_customers, \"Item_Position\"] = 0\n",
        "sales_orders.loc[index_anti_join_saleOrders, \"Item_Position\"] = 0\n",
        "\n",
        "# Merge all data in a final dataframe\n",
        "df = pd.merge(customers, sales_orders,  how='outer', left_on=['Sales_Order','Item_Position'], right_on = ['Sales_Order','Item_Position'])\n",
        "df = pd.merge(df, classification,  how='outer', left_on=['Customer_ID'], right_on = ['Customer_ID'])\n",
        "df = pd.merge(df, sales_orders_header,  how='outer', left_on=['Sales_Order'], right_on = ['Sales_Order'])\n",
        "df = pd.merge(df, service_map ,  how='outer', left_on=['Material_Class'], right_on = ['MATKL_service'])\n",
        "df = pd.merge(df, business_units ,  how='outer', left_on=['Cost_Center'], right_on = ['Cost_Center'])"
      ],
      "metadata": {
        "id": "euS-SMXOXXQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['Net_Value','YHKOKRS' ], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "QwOTbaEAuqh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling the Data"
      ],
      "metadata": {
        "id": "0fb4dKc-ZJhj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to remove Nan values. "
      ],
      "metadata": {
        "id": "S83JqSpfbYLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing rows that have Nan values in the significant columns\n",
        "df_nan_row_removed = df.dropna(subset=['Sales_Order', 'Customer_ID','Item_Position'])\n",
        "\n",
        "# Dropping columns that do not contribute to Reseller identification information\n",
        "df_with_important_columns = df_nan_row_removed.drop('MATKL_service',axis=1)\n",
        "\n",
        "# Find the columns that have Nan values as an anomaly, 'Reseller','Test_set_id','Release_Date' might include nan values due to the nature of the data\n",
        "nan_anomaly_cols = [i for i in df_with_important_columns.columns.tolist() if i not in ['Reseller','Test_set_id','Release_Date']]\n",
        "df_nan_anomaly_removed_cols = df_with_important_columns.dropna(subset=nan_anomaly_cols)\n",
        "\n",
        "df_anomaly_handled = df_nan_anomaly_removed_cols"
      ],
      "metadata": {
        "id": "_tVxpBVjZIg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to handle date/time data to use them as an informative feature for the detection of the Reseller companies. \n",
        "\n"
      ],
      "metadata": {
        "id": "KKW68FDybUVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identifiying date columns\n",
        "date_cols = ['Creation_Date','Release_Date']\n",
        "\n",
        "# Identifiying the indicies that have Release Date col as Nan and replacing those with Creating Date value to not lose data\n",
        "date_nan_indices = df_anomaly_handled.loc[df_anomaly_handled.Release_Date.isna()].index.values.tolist()\n",
        "df_anomaly_handled.loc[date_nan_indices, 'Release_Date'] = df_anomaly_handled.loc[date_nan_indices, 'Creation_Date']\n",
        "df_anomaly_handled['Release_Date'] = pd.to_datetime(df_anomaly_handled['Release_Date'], utc=True)\n",
        "\n",
        "# Converting object type date&time cols to datetime objects and extracting meaningful information\n",
        "df_anomaly_handled['week_number_cd'] = pd.to_datetime(df_anomaly_handled['Creation_Date']).dt.week\n",
        "df_anomaly_handled['month_number_cd'] = pd.to_datetime(df_anomaly_handled['Creation_Date']).dt.month\n",
        "df_anomaly_handled['year_number_cd'] = pd.to_datetime(df_anomaly_handled['Creation_Date']).dt.year\n",
        "df_anomaly_handled['season_number_cd'] = pd.to_datetime(df_anomaly_handled['Creation_Date']).dt.quarter\n",
        "\n",
        "df_anomaly_handled['week_number_rd'] = pd.to_datetime(df_anomaly_handled['Release_Date']).dt.week\n",
        "df_anomaly_handled['month_number_rd'] = pd.to_datetime(df_anomaly_handled['Release_Date']).dt.month\n",
        "df_anomaly_handled['year_number_rd'] = pd.to_datetime(df_anomaly_handled['Release_Date']).dt.year\n",
        "df_anomaly_handled['season_number_rd'] = pd.to_datetime(df_anomaly_handled['Release_Date']).dt.quarter\n",
        "\n",
        "# Create a new feature named 'time_diff' which shows the diff. between release and creation date. this can be an important attribute. \n",
        "df_anomaly_handled['time_diff'] = pd.to_datetime(df_anomaly_handled['Release_Date'],utc=True).dt.floor('d') - pd.to_datetime(df_anomaly_handled['Creation_Date'],utc=True).dt.floor('d')\n",
        "df_anomaly_handled['time_diff'] = df_anomaly_handled['time_diff'].dt.days\n",
        "\n",
        "df_anomaly_handled.drop(date_cols, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "VVojrQL2bT5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identifiying categorical data columns so that we can encode them to use with ML algorithms\n",
        "df_encoded = df_anomaly_handled.copy()\n",
        "categorical_cols = ['Cost_Center', 'Type', 'Customer_ID', 'Material_Code', 'Cost_Center', 'Sales_Organization', 'Creator', 'Document_Type', 'Delivery', 'Business_Unit']\n",
        "\n",
        "labelEncoder = LabelEncoder()\n",
        "for col in categorical_cols:\n",
        "  df_encoded[col] = labelEncoder.fit_transform(df_encoded[col])\n"
      ],
      "metadata": {
        "id": "f2gfwPV2f3Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new feature named 'Suspicious' out of Type column\n",
        "append_data = []\n",
        "for i,j in df_encoded.groupby(['Sales_Order','Item_Position']):\n",
        "  if len(pd.DataFrame(j)['Customer_ID'].unique()) != 1:\n",
        "    temp = pd.DataFrame(j)[['Sales_Order','Item_Position','Type', 'Customer_ID']]\n",
        "    temp['Suspicious'] = 1.0\n",
        "    append_data.append(temp[['Sales_Order','Item_Position', 'Suspicious']])\n",
        "\n",
        "suspicious_columns = pd.concat(append_data)\n",
        "\n",
        "df_final = pd.merge(df_encoded, suspicious_columns,  how='outer', left_on=['Sales_Order','Item_Position'], right_on = ['Sales_Order','Item_Position'])\n",
        "df_final['Suspicious'] = df_final['Suspicious'].fillna(0)"
      ],
      "metadata": {
        "id": "ihMbFLgsg2CL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Selection using Correlation"
      ],
      "metadata": {
        "id": "y-wZYZS3kBZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract correlation matrix and visualize it\n",
        "corr_matrix = df_final[[c for c in df_final.columns if c not in ['Reseller', 'Test_set_id', 'track_id','Sales_Order', 'Type']]].corr().abs()\n",
        "\n",
        "#plt.figure(figsize=(24,16))\n",
        "#sns.heatmap(corr_matrix, annot=True, cmap=plt.cm.Reds)\n",
        "#plt.show()\n",
        "\n",
        "def drop_highly_correlated(corr_matrix, threshold, df):\n",
        "    \n",
        "    # As correlation matrix is diagonal by its nature we extract upper triangle\n",
        "    upper_tri_area = np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool)\n",
        "    upper_tri = corr_matrix.where(upper_tri_area)\n",
        "    # Choose highly correlated columns according to threshold\n",
        "    corr_cols = [col for col in upper_tri.columns if any(upper_tri[col] >= threshold)]\n",
        "    # Drop chosen columns\n",
        "    df = df.drop(df[corr_cols], axis=1, inplace=False)\n",
        "    \n",
        "    return df\n",
        "\n",
        "df_selected = drop_highly_correlated(corr_matrix, 0.40, df_final[[c for c in df_final.columns if c not in ['Reseller', 'Test_set_id', 'track_id','Sales_Order', 'Type']]])"
      ],
      "metadata": {
        "id": "I4LclHEbkA9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# update df_final by dropping not selected columns\n",
        "\n",
        "preserved_columns = df_selected.columns.tolist()\n",
        "for c in ['Reseller', 'Test_set_id', 'track_id','Sales_Order', 'Type']:\n",
        "  preserved_columns.append(c)\n",
        "\n",
        "df_selected = df_final[preserved_columns]"
      ],
      "metadata": {
        "id": "sDS5EHVykgQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a standardized dataset\n",
        "to_be_transformed_cols = ['Num_Items', 'Net_Value_Corrected', 'time_diff']\n",
        "sc = StandardScaler()\n",
        "df_standardized = df_selected.copy()\n",
        "df_standardized[to_be_transformed_cols] = sc.fit_transform(df_standardized[to_be_transformed_cols])\n",
        "#df_standardized.describe().T"
      ],
      "metadata": {
        "id": "WeuzFFygtY9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split Data into Train, Validation, Test sets"
      ],
      "metadata": {
        "id": "oHmBj9iojP7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_selected_1 = df_selected[df_selected.Reseller == 1]\n",
        "df_selected_0 = df_selected[df_selected.Reseller == 0].sample(n=len(df_selected_1))\n",
        "df_selected_final = df_selected_1.append(df_selected_0, ignore_index=False)"
      ],
      "metadata": {
        "id": "siDpRt_wPp8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = df_selected[df_selected.Reseller.isnull()]\n",
        "train_and_val = df_selected_final[~(df_selected_final.Reseller.isnull())]\n",
        "train, val = train_test_split(train_and_val, test_size=0.1, random_state=RANDOM_SEED, stratify=train_and_val.Reseller)\n",
        "# The X,y train splits to train the models\n",
        "X, y = train.drop(['Reseller', 'Test_set_id', 'track_id','Sales_Order', 'Type', 'Customer_ID'], axis=1), train.Reseller\n",
        "# The validation set to evaluate the fitted models\n",
        "X_val, y_val = val.drop(['Reseller', 'Test_set_id', 'track_id','Sales_Order','Type', 'Customer_ID'], axis=1), val.Reseller\n",
        "# Test set provided as test set\n",
        "X_test = test.drop(['Reseller', 'Test_set_id', 'track_id','Sales_Order','Type','Customer_ID'], axis=1)"
      ],
      "metadata": {
        "id": "f1BIzl_pil02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Development"
      ],
      "metadata": {
        "id": "WGwrh9c8kLJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "from statistics import multimode\n",
        "from functools import reduce\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# We classify each sales order as if it is a Reseller action or not, then based on the each Customer sales order we perform majority voting \n",
        "def mod_to_classify(series):\n",
        "  result = multimode(series.to_list()) \n",
        "  if(len(result) > 1):\n",
        "    return 0\n",
        "  else:\n",
        "    return result[0]\n",
        "\n",
        "def BAC_metric(y_true, y_pred):\n",
        "  tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "  # Sensitivity\n",
        "  sensitivity = tp/(tp+fn)\n",
        "  # Specificity\n",
        "  specificity = tn/(tn+fp)  \n",
        "  bac = (sensitivity + specificity)/(2)\n",
        "  return bac"
      ],
      "metadata": {
        "id": "tIEpoyb7kVBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "random_forest_clf = RandomForestClassifier(random_state=RANDOM_SEED)\n",
        "random_forest_clf.fit(X, y)"
      ],
      "metadata": {
        "id": "OzY-Xr1GkNR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "xgb_classifier = xgb.XGBClassifier(objective= 'binary:logistic',\n",
        "    nthread=4,random_state=RANDOM_SEED)"
      ],
      "metadata": {
        "id": "RO2SXQmJa41p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = xgb_classifier"
      ],
      "metadata": {
        "id": "hhHnkRAHa6yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRID SEARCH\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters = {\n",
        "    'max_depth': range (5, 15, 2),\n",
        "    'n_estimators': range(60, 200,20),\n",
        "    'learning_rate': [0.1, 0.01, 0.05],\n",
        "    'tree_method': ['gpu_hist']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb_classifier,\n",
        "    param_grid=parameters,\n",
        "    scoring = 'f1',\n",
        "    n_jobs = 2,\n",
        "    cv=2,\n",
        "    verbose=True,\n",
        "  \n",
        ")\n",
        "\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "SNmLaKhPp7ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "xgb_classifier = xgb.XGBClassifier(max_depth=16, n_estimators=180, nthread=4, random_state=44)\n",
        "xgb_classifier.fit(X,y)"
      ],
      "metadata": {
        "id": "mg6AZWQhlfy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = xgb_classifier"
      ],
      "metadata": {
        "id": "sUWbQiOMqmds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation set evaluation\n",
        "y_val_pred = clf.predict(X_val)     \n",
        "val['y_pred'] = y_val_pred\n",
        "val_pred_results = val.groupby('track_id').agg({'y_pred': [mod_to_classify]})\n",
        "val_gt_results = val.groupby('track_id').agg({'Reseller': [mod_to_classify]})\n",
        "val_acc = accuracy_score(val_gt_results, val_pred_results)\n",
        "val_bac_score = BAC_metric(y_true=val_gt_results, y_pred=val_pred_results)"
      ],
      "metadata": {
        "id": "ekbJV8eGlIvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_acc, val_bac_score"
      ],
      "metadata": {
        "id": "WEhCWPESpQXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test**"
      ],
      "metadata": {
        "id": "rXIzeg7cTNGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test set result \n",
        "y_test_pred = clf.predict(X_test)\n",
        "X_test['track_id'] = test['track_id']\n",
        "X_test['y_pred'] = y_test_pred \n",
        "grouped_results_test_pred = X_test.groupby('track_id').agg({'y_pred': [mod_to_classify]})\n",
        "prediction = [int(i[0]) for i in grouped_results_test_pred.y_pred.values.tolist()]\n",
        "sub_data = pd.DataFrame({'prediction': prediction }, index=[int(i) for i in grouped_results_test_pred.index])"
      ],
      "metadata": {
        "id": "a-Px-aWf4M3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_data.to_csv('predictions_HU_Deers_11.csv', index_label='id')"
      ],
      "metadata": {
        "id": "_M-Onqg3vVZM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}